{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5d458a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7851bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'El tamaño de embedding debe ser divisible entre num_heads.'\n",
    "        \n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # 1) Proyectamos Q, K, V\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 2) Calculamos los scores de atención\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 3) Mezclamos valores\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "        \n",
    "        # 4) Reorganizamos y aplicamos la proyección de salida\n",
    "        weighted_values = weighted_values.transpose(1, 2).contiguous()\n",
    "        weighted_values = weighted_values.view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        out = self.W_o(weighted_values)\n",
    "        \n",
    "        return out, attention\n",
    "\n",
    "class PositionFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "class EncoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # 1) Self-attention\n",
    "        attn_out, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout1(attn_out)  # skip connection\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # 2) Feed-forward\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = x + self.dropout2(ffn_out)  # skip connection\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderSubLayer(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        pos_embed = torch.zeros(max_seq_len, d_model)\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pos_embed[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        pos_embed[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        \n",
    "        pos_embed = pos_embed.unsqueeze(0)  # (1, max_seq_len, d_model)\n",
    "        self.register_buffer('pos_embed', pos_embed)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pos_embed[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed898700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderClassifierWithCLS(nn.Module):\n",
    "    \"\"\"\n",
    "    Agrega un token [CLS] entrenable al inicio de la secuencia.\n",
    "    La salida de la posición 0 (ese [CLS]) se usa para la clasificación final.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, input_dim, num_classes, max_seq_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Proyectamos la entrada a d_model (ej. 80 -> 64)\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Token CLS entrenable\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, d_model))\n",
    "        \n",
    "        # Embedding posicional\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_seq_len)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        \n",
    "        # Capa final de clasificación\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 1) Proyectamos la entrada a d_model\n",
    "        x = self.input_projection(x) # (B, seq_len, d_model)\n",
    "        \n",
    "        # 2) Construimos un batch de [CLS] tokens -> (B, 1, d_model)\n",
    "        cls_tokens = self.cls_token.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # 3) Concatenamos el CLS al inicio de la secuencia\n",
    "        x = torch.cat([cls_tokens, x], dim=1) # (B, seq_len+1, d_model)\n",
    "        \n",
    "        # 4) Sumamos el embedding posicional\n",
    "        x = self.pos_embedding(x) # (B, seq_len+1, d_model)\n",
    "        \n",
    "        # 5) Pasamos por el encoder\n",
    "        x = self.encoder(x, mask) # (B, seq_len+1, d_model)\n",
    "        \n",
    "        # 6) Tomamos la posición 0 (el token CLS)\n",
    "        cls_vector = x[:, 0, :] # (B, d_model)\n",
    "        \n",
    "        # 7) Clasificación\n",
    "        logits = self.classifier(cls_vector) # (B, num_classes)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "066a9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticTimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Genera datos sintéticos para clasificación con series temporales.\n",
    "    Cada muestra es (seq_len, input_dim).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples=2000, seq_len=50, input_dim=80, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Generamos X aleatorios: (n_samples, seq_len, input_dim)\n",
    "        self.X = torch.randn(n_samples, seq_len, input_dim)\n",
    "        \n",
    "        # Generamos etiquetas aleatorias (0..num_classes-1)\n",
    "        self.Y = torch.randint(0, num_classes, (n_samples,))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f8f8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros del dataset sintético\n",
    "n_samples  = 2000\n",
    "seq_len    = 50\n",
    "input_dim  = 80   \n",
    "num_classes= 2\n",
    "    \n",
    "dataset = SyntheticTimeSeries(\n",
    "    n_samples=n_samples,\n",
    "    seq_len=seq_len,\n",
    "    input_dim=input_dim,\n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84cf0970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de la primera instancia: torch.Size([50, 80])\n",
      "Datos de la primera instancia (X):\n",
      " tensor([[-1.4244, -0.9550,  0.1341,  ..., -0.5628,  0.6911,  1.0790],\n",
      "        [ 0.3659,  0.2115, -0.3337,  ...,  1.6594, -0.1341, -0.2431],\n",
      "        [-1.4377,  1.5517, -0.6442,  ..., -0.2413,  1.1100,  0.2479],\n",
      "        ...,\n",
      "        [ 1.0732,  0.4124,  0.6573,  ...,  0.5138, -0.6348,  0.8971],\n",
      "        [ 1.1215,  0.5846,  1.0779,  ..., -1.5125, -0.8529,  2.1013],\n",
      "        [ 1.1741,  0.8247, -0.2051,  ...,  0.7012,  0.1913,  0.2018]])\n",
      "Etiqueta (Y): tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos la primera instancia (x=features, y=label)\n",
    "first_x, first_y = dataset[0]\n",
    "\n",
    "# Imprimimos en consola la forma y los datos\n",
    "print(\"Shape de la primera instancia:\", first_x.shape)\n",
    "print(\"Datos de la primera instancia (X):\\n\", first_x)\n",
    "print(\"Etiqueta (Y):\", first_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "141917a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_x)  # (batch_size, num_classes)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            logits = model(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Precisión\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88d7dd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Train Loss: 0.7227 | Val Loss: 0.7111 | Val Acc: 46.25%\n",
      "Epoch [2/5] Train Loss: 0.7021 | Val Loss: 0.6906 | Val Acc: 53.50%\n",
      "Epoch [3/5] Train Loss: 0.7002 | Val Loss: 0.6917 | Val Acc: 53.75%\n",
      "Epoch [4/5] Train Loss: 0.6930 | Val Loss: 0.7020 | Val Acc: 46.50%\n",
      "Epoch [5/5] Train Loss: 0.6780 | Val Loss: 0.7077 | Val Acc: 54.00%\n",
      "¡Entrenamiento finalizado!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    SEED = 42\n",
    "    torch.manual_seed(SEED)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    # Parámetros del dataset sintético\n",
    "    n_samples  = 2000\n",
    "    seq_len    = 50\n",
    "    input_dim  = 80   # Cada paso tiene 80 features\n",
    "    num_classes= 2    # Clasificación binaria (0 o 1)\n",
    "\n",
    "    # Parámetros del Transformer\n",
    "    d_model   = 64\n",
    "    num_heads = 4\n",
    "    d_ff      = 128\n",
    "    num_layers= 2\n",
    "    max_seq_len = seq_len + 1  # sumamos 1 porque ahora hay CLS token\n",
    "    \n",
    "    # Hiperparámetros de entrenamiento\n",
    "    batch_size = 32\n",
    "    lr         = 1e-3\n",
    "    epochs     = 5\n",
    "    \n",
    "    # 1) Dataset y DataLoader\n",
    "    dataset = SyntheticTimeSeriesDataset(\n",
    "        n_samples=n_samples,\n",
    "        seq_len=seq_len,\n",
    "        input_dim=input_dim,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size   = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # 2) Modelo con CLS real\n",
    "    model = TransformerEncoderClassifierWithCLS(\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        num_layers=num_layers,\n",
    "        input_dim=input_dim,\n",
    "        num_classes=num_classes,\n",
    "        max_seq_len=max_seq_len, # recordatorio: 1 slot extra para el CLS\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    # 3) Definimos pérdida y optimizador\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # 4) Entrenamiento\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "        \n",
    "        print(f\"Epoch [{epoch}/{epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | \"\n",
    "              f\"Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    print(\"¡Entrenamiento finalizado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb39968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0649fbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
