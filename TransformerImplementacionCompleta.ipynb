{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df3e364",
   "metadata": {},
   "source": [
    "# Transformer - Attention is all you need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b93ad38",
   "metadata": {},
   "source": [
    "## Máscaras en el Transformer: Padding y Causal Mask\n",
    "\n",
    "### ¿Por qué se necesitan máscaras?\n",
    "\n",
    "En un modelo Transformer, las **máscaras** se usan para controlar qué partes de la secuencia pueden \"verse\" entre sí durante la auto-atención. Esto es fundamental tanto en el **encoder** como en el **decoder**, pero por diferentes razones:\n",
    "\n",
    "- **Encoder** → necesita una **padding mask** para ignorar los tokens de relleno (`<PAD>`).\n",
    "- **Decoder** → necesita tanto:\n",
    "  - una **padding mask**,\n",
    "  - como una **máscara causal (look-ahead mask)** que impide ver tokens del futuro durante la generación.\n",
    "\n",
    "---\n",
    "\n",
    "| Tipo de Máscara         | Código / Ejemplo                                                                                     | Forma                                          | Descripción                                                                                                     |\n",
    "|-------------------------|------------------------------------------------------------------------------------------------------|------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|\n",
    "| **Padding Mask (source)**   | `(source != 0).unsqueeze(1).unsqueeze(2)`                                                           | `(batch_size, 1, 1, source_seq_len)`             | Ignora los tokens de padding (0) en el input del encoder.                                                       |\n",
    "| **Padding Mask (target)**   | `(target != 0).unsqueeze(1).unsqueeze(2)`                                                           | `(batch_size, 1, 1, target_seq_len)`             | Ignora los tokens de padding en el input del decoder.                                                           |\n",
    "| **Máscara Causal (Look-Ahead)** | `torch.tril(torch.ones(1, size, size)).bool()` <br> *(con `size = target.size(1)`)*                  | `(1, target_seq_len, target_seq_len)`            | Triangular inferior: permite que el token en posición *i* vea solo los tokens hasta la posición *i*.             |\n",
    "| **Target Mask Combinada**   | `(target != 0).unsqueeze(1).unsqueeze(2) & no_mask` <br> *(donde `no_mask` es la máscara causal)*     | `(batch_size, 1, target_seq_len, target_seq_len)` | Combina la máscara de padding y la causal para el decoder, usando broadcasting para ajustar las dimensiones.     |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de291687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7ff889e0f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Semilla de reproducibilidad\n",
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2fbe4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "270e528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6aff51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() \n",
    "                             * (-math.log(10000.0)/d_model))\n",
    "        \n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0,1) # Para hacer broadcasting con x.shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Broadcasting automático\n",
    "        # x: (seq_len, batch_size, d_model)\n",
    "        # pos_embed_matrix: (seq_len, d_model)\n",
    "        # resultado: (seq_len, batch_size, d_model) + (seq_len, 1, d_model) = (seq_len, batch_size, d_model)\n",
    "        # print(self.pos_embed_matrix.shape)\n",
    "        # print(x.shape)\n",
    "        return x + self.pos_embed_matrix[:x.size(0), :]\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8): # d_model tiene que ser divisible entre num_heads. d_v = 512/8 = 64. (8*64=512). Siendo 512 el tamaño del embedding y la concatenación de las 8 cabezas igual al tamaño del embedding\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
    "        \n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model) # En lugar de hacer 8 de 512x64 hacemos una de 512x512 (más eficiente)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        '''\n",
    "        Q, K, V -> [batch_size, seq_len, num_heads*d_k]\n",
    "        Después de view Q: (batch_size, 10, 8, 64)\n",
    "        Luego de transpose se reorganiza a (batch_size, 8, 10, 64) para aplicar atención\n",
    "        '''\n",
    "        Q = self.W_q(Q).view(batch_size, -1, num_heads, self.d_k).transpose(1,2) # Partimos la dimension de 512 en 8 cabezas de 64. Cada token tiene 8 sub-vectores de 64 → 1 por cabeza\n",
    "        K = self.W_k(K).view(batch_size, -1, num_heads, self.d_k).transpose(1,2)\n",
    "        V = self.W_v(V).view(batch_size, -1, num_heads, self.d_k).transpose(1,2)\n",
    "        \n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "        \n",
    "        weighted_values = weighted_values.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads*self.d_k) # (batch_size, num_heads, seq_len, d_k)\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "    def scale_dot_product(self, Q, K, V, mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None: # En el Encoder para el padding, en el Decoder para el padding y para no ver el futuro del output\n",
    "            scores = scores.masked_fill(mask == 0, -1e9) # Para que al aplicar softmax den probabilidades de 0\n",
    "            # scores.shape = (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        attention = F.softmax(scores, dim=-1) # dim=-1 normaliza por filas\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "    \n",
    "class PositionFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "    \n",
    "    \n",
    "class EncoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_score, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout1(attn_score) # Skip connection\n",
    "        x = self.norm1(x) # Normalización\n",
    "        x = x + self.dropout2(self.ffn(x)) # Skip connection\n",
    "        return self.norm2(x) # Normalización \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]) # Nx capas secuenciales\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # mask para el padding\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n",
    "        attention_score, _ = self.self_attn(x, x, x, target_mask)\n",
    "        x = x + self.dropout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)\n",
    "        x = x + self.dropout2(encoder_attn)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        return self.norm3(x)\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
    "        # cross-attention\n",
    "        # Necesitamos el encoder_mask para no atender a los maskings\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7da34f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, input_vocab_size, target_vocab_size,\n",
    "                max_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        # d_model: Tamaño de los embeddings\n",
    "        # num_heads: Número de cabezas paralelas de atención\n",
    "        # d_ff: Tamaño de las redes neuronales Feed-Forward\n",
    "        # num_layers: Número de capas secuenciales tanto para el encoder como para el decoder\n",
    "        # input_vocab_size\n",
    "        # target_vocab_size\n",
    "        # max_len: Tamaño de la ventana de contexto\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        # Encoder mask\n",
    "        source_mask, target_mask = self.mask(source, target)\n",
    "        # Embedding and positional Encoding\n",
    "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim) # Técnica de escalado para normalizar los valores de los embeddings\n",
    "        source = self.pos_embedding(source)\n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        \n",
    "        # Decoder embedding and positional encoding\n",
    "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
    "        target = self.pos_embedding(target)\n",
    "        # Decoder\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "        \n",
    "        return self.output_layer(output)\n",
    "        \n",
    "    def mask(self, source, target):\n",
    "        # El token de 0 es de padding\n",
    "        # El resto o bien son tokens especiales (<SOS>, <EOS>) o bien palabras (Aqui cada palabra equivale a un token)\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "        size = target.size(1)  # La dimensión 1 representa la longitud de la secuencia (max_seq_len)\n",
    "        no_mask = torch.tril(torch.ones(1, size, size, device=device)).bool() # Para evitar ver palabras futuras que aún no se han generado\n",
    "        target_mask = target_mask & no_mask # Broadcasting automático  # (B, 1, T, T)\n",
    "        return source_mask, target_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be34cde",
   "metadata": {},
   "source": [
    "### Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bc68dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len_source = 10\n",
    "seq_len_target = 10\n",
    "batch_size = 2\n",
    "input_vocab_size = 50\n",
    "target_vocab_size = 50\n",
    "\n",
    "source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))\n",
    "target = torch.randint(1, target_vocab_size, (batch_size, seq_len_target))\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "\n",
    "model = Transformer(d_model, num_heads, d_ff, num_layers, input_vocab_size, target_vocab_size,\n",
    "                max_len=MAX_SEQ_LEN, dropout=0.1)\n",
    "\n",
    "model = model.to(device)\n",
    "source = source.to(device)\n",
    "target = target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "280b3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a2062b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 50])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3c127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7195764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/vmympyyx6dd0fbfl7d2k9cv00000gn/T/ipykernel_49965/3934573465.py:7: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(PATH, sep=';', header=None)\n",
      "/var/folders/y9/vmympyyx6dd0fbfl7d2k9cv00000gn/T/ipykernel_49965/3934573465.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eng_spa_cols['length'] = eng_spa_cols['en'].str.len()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo original\n",
    "PATH = './Parejas de oraciones en InglésEspañol - 2025-04-01.csv'\n",
    "\n",
    "# 1. Cargar el CSV ignorando las columnas vacías\n",
    "df = pd.read_csv(PATH, sep=';', header=None)\n",
    "\n",
    "# 2. Extraer solo las columnas de texto: inglés (col 1) y español (col 3)\n",
    "eng_spa_cols = df.iloc[:, [1, 3]]\n",
    "eng_spa_cols.columns = ['en', 'es']  # (opcional pero más legible)\n",
    "\n",
    "# 3. Calcular la longitud de la oración en inglés\n",
    "eng_spa_cols['length'] = eng_spa_cols['en'].str.len()\n",
    "\n",
    "# 4. Ordenar por longitud (frases más cortas primero)\n",
    "eng_spa_cols = eng_spa_cols.sort_values(by='length')\n",
    "\n",
    "# 5. Eliminar la columna de longitud\n",
    "eng_spa_cols = eng_spa_cols.drop(columns=['length'])\n",
    "\n",
    "# 6. Guardar el resultado como archivo de texto tabulado\n",
    "output_file_path = './eng-spa.txt'\n",
    "eng_spa_cols.to_csv(output_file_path, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e0423df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165605</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159526</th>\n",
       "      <td>Ah!</td>\n",
       "      <td>¡Anda!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95238</th>\n",
       "      <td>Go!</td>\n",
       "      <td>¡Fuera!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95239</th>\n",
       "      <td>Go!</td>\n",
       "      <td>¡Ya!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162092</th>\n",
       "      <td>OK.</td>\n",
       "      <td>Bueno.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         en       es\n",
       "165605  Go.    Vaya.\n",
       "159526  Ah!   ¡Anda!\n",
       "95238   Go!  ¡Fuera!\n",
       "95239   Go!     ¡Ya!\n",
       "162092  OK.   Bueno."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_spa_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4bcd5607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145656</th>\n",
       "      <td>As far as I understand despite my limited know...</td>\n",
       "      <td>Hasta donde comprendo a pesar de mi poco conoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161607</th>\n",
       "      <td>Three hours later, the King was loitering arou...</td>\n",
       "      <td>Tres horas después, el rey estaba merodeando p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265346</th>\n",
       "      <td>In Japanese, conjugation is fundamental to pie...</td>\n",
       "      <td>En japonés, la conjugación es fundamental para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73305</th>\n",
       "      <td>There is no such thing, at this stage of the w...</td>\n",
       "      <td>No existe tal cosa, en esta etapa de la histor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250695</th>\n",
       "      <td>I know some things about reading when it comes...</td>\n",
       "      <td>Sé algunas cosas sobre la lectura cuando se tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       en  \\\n",
       "145656  As far as I understand despite my limited know...   \n",
       "161607  Three hours later, the King was loitering arou...   \n",
       "265346  In Japanese, conjugation is fundamental to pie...   \n",
       "73305   There is no such thing, at this stage of the w...   \n",
       "250695  I know some things about reading when it comes...   \n",
       "\n",
       "                                                       es  \n",
       "145656  Hasta donde comprendo a pesar de mi poco conoc...  \n",
       "161607  Tres horas después, el rey estaba merodeando p...  \n",
       "265346  En japonés, la conjugación es fundamental para...  \n",
       "73305   No existe tal cosa, en esta etapa de la histor...  \n",
       "250695  Sé algunas cosas sobre la lectura cuando se tr...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_spa_cols.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0e6bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './eng-spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "634adf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "eng_spa_pairs = [line.strip().split('\\t') for line in lines if '\\t' in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "20372fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Go.', 'Vaya.'],\n",
       " ['Ah!', '¡Anda!'],\n",
       " ['Go!', '¡Fuera!'],\n",
       " ['Go!', '¡Ya!'],\n",
       " ['OK.', 'Bueno.'],\n",
       " ['Go!', '¡Sal!'],\n",
       " ['Go!', '¡Ve!'],\n",
       " ['Hi.', 'Hola.'],\n",
       " ['Go!', 'Vete'],\n",
       " ['No.', 'No.']]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_spa_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "16af8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences = [pair[0] for pair in eng_spa_pairs]\n",
    "spa_sentences = [pair[1] for pair in eng_spa_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aadd8261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go.', 'Ah!', 'Go!', 'Go!', 'OK.', 'Go!', 'Go!', 'Hi.', 'Go!', 'No.']\n",
      "['Vaya.', '¡Anda!', '¡Fuera!', '¡Ya!', 'Bueno.', '¡Sal!', '¡Ve!', 'Hola.', 'Vete', 'No.']\n"
     ]
    }
   ],
   "source": [
    "print(eng_sentences[:10])\n",
    "print(spa_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "776f9558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[á]+\", \"a\", sentence)\n",
    "    sentence = re.sub(r\"[é]+\", \"e\", sentence)\n",
    "    sentence = re.sub(r\"[í]+\", \"i\", sentence)\n",
    "    sentence = re.sub(r\"[ó]+\", \"o\", sentence)\n",
    "    sentence = re.sub(r\"[ú]+\", \"u\", sentence)\n",
    "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<sos> ' + sentence + ' <eos>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7c58f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = '¿Hola @ cómo estás? 123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2e32df54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Hola @ cómo estás? 123\n",
      "<sos> hola como estas <eos>\n"
     ]
    }
   ],
   "source": [
    "print(s1)\n",
    "print(preprocess_sentence(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2a9772b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences = [preprocess_sentence(sentence) for sentence in eng_sentences]\n",
    "spa_sentences = [preprocess_sentence(sentence) for sentence in spa_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "41ebebe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> vaya <eos>',\n",
       " '<sos> anda <eos>',\n",
       " '<sos> fuera <eos>',\n",
       " '<sos> ya <eos>',\n",
       " '<sos> bueno <eos>',\n",
       " '<sos> sal <eos>',\n",
       " '<sos> ve <eos>',\n",
       " '<sos> hola <eos>',\n",
       " '<sos> vete <eos>',\n",
       " '<sos> no <eos>']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spa_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f7745552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    words = [word for sentence in sentences for word in sentence.split()]\n",
    "    word_count = Counter(words)\n",
    "    sorted_word_counts = sorted(word_count.items(), key=lambda x:x[1], reverse=True)\n",
    "    word2idx = {word: idx for idx, (word, _) in enumerate(sorted_word_counts, 2)}\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "75e2fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_word2idx, eng_idx2word = build_vocab(eng_sentences)\n",
    "spa_word2idx, spa_idx2word = build_vocab(spa_sentences)\n",
    "eng_vocab_size = len(eng_word2idx)\n",
    "spa_vocab_size = len(spa_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "af0bbe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27945 47492\n"
     ]
    }
   ],
   "source": [
    "print(eng_vocab_size, spa_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "58ebd98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngSpaDataset(Dataset):\n",
    "    def __init__(self, eng_sentences, spa_sentences, eng_word2idx, spa_word2idx):\n",
    "        self.eng_sentences = eng_sentences\n",
    "        self.spa_sentences = spa_sentences\n",
    "        self.eng_word2idx = eng_word2idx\n",
    "        self.spa_word2idx = spa_word2idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.eng_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        eng_sentence = self.eng_sentences[idx]\n",
    "        spa_sentence = self.spa_sentences[idx]\n",
    "        # return tokens idxs\n",
    "        eng_idxs = [self.eng_word2idx.get(word, self.eng_word2idx['<unk>']) for word in eng_sentence.split()]\n",
    "        spa_idxs = [self.spa_word2idx.get(word, self.spa_word2idx['<unk>']) for word in spa_sentence.split()]\n",
    "        \n",
    "        return torch.tensor(eng_idxs), torch.tensor(spa_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "000832ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    eng_batch, spa_batch = zip(*batch)\n",
    "    eng_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in eng_batch]\n",
    "    spa_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in spa_batch]\n",
    "    eng_batch = torch.nn.utils.rnn.pad_sequence(eng_batch, batch_first=True, padding_value=0)\n",
    "    spa_batch = torch.nn.utils.rnn.pad_sequence(spa_batch, batch_first=True, padding_value=0)\n",
    "    return eng_batch, spa_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "724c132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_function, optimiser, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0 \n",
    "        for i, (eng_batch, spa_batch) in enumerate(dataloader):\n",
    "            eng_batch = eng_batch.to(device)\n",
    "            spa_batch = spa_batch.to(device)\n",
    "            # Decoder preprocessing\n",
    "            target_input = spa_batch[:, :-1]\n",
    "            target_output = spa_batch[:, 1:].contiguous().view(-1)\n",
    "            # Zero grads\n",
    "            optimiser.zero_grad()\n",
    "            # run model\n",
    "            output = model(eng_batch, target_input)\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            # loss\\\n",
    "            loss = loss_function(output, target_output)\n",
    "            # gradient and update parameters\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss/len(dataloader)\n",
    "        print(f'Epoch: {epoch}/{epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "351904c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "638763c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(d_model=512, num_heads=8, d_ff=2048, num_layers=6,\n",
    "                    input_vocab_size=eng_vocab_size, target_vocab_size=spa_vocab_size,\n",
    "                    max_len=MAX_SEQ_LEN, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ddcc2a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ed6700d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y9/vmympyyx6dd0fbfl7d2k9cv00000gn/T/ipykernel_49965/1945126858.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/y9/vmympyyx6dd0fbfl7d2k9cv00000gn/T/ipykernel_49965/2336624727.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, loss_function, optimiser, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# gradient and update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, dataloader, loss_function, optimiser, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c7e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence, word2idx):\n",
    "    return [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
    "\n",
    "def indices_to_sentence(indices, idx2word):\n",
    "    return ' '.join([idx2word[idx] for idx in indices if idx in idx2word and idx2word[idx] != '<pad>'])\n",
    "\n",
    "def translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    model.eval()\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    input_indices = sentence_to_indices(sentence, eng_word2idx)\n",
    "    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    # Initialize the target tensor with <sos> token\n",
    "    tgt_indices = [spa_word2idx['<sos>']]\n",
    "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_tensor, tgt_tensor)\n",
    "            output = output.squeeze(0)\n",
    "            next_token = output.argmax(dim=-1)[-1].item()\n",
    "            tgt_indices.append(next_token)\n",
    "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "            if next_token == spa_word2idx['<eos>']:\n",
    "                break\n",
    "\n",
    "    return indices_to_sentence(tgt_indices, spa_idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translations(model, sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    for sentence in sentences:\n",
    "        translation = translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len, device)\n",
    "        print(f'Input sentence: {sentence}')\n",
    "        print(f'Traducción: {translation}')\n",
    "        print()\n",
    "\n",
    "# Example sentences to test the translator\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am learning artificial intelligence.\",\n",
    "    \"Artificial intelligence is great.\",\n",
    "    \"Good night!\"\n",
    "]\n",
    "\n",
    "# Assuming the model is trained and loaded\n",
    "# Set the device to 'cpu' or 'cuda' as needed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Evaluate translations\n",
    "evaluate_translations(model, test_sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557b61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "84c71325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar pesos\n",
    "import torch\n",
    "\n",
    "# --- Después de entrenar tu modelo ---\n",
    "torch.save(model.state_dict(), \"transformer_weights.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
