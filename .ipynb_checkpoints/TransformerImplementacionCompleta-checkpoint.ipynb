{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fe64174",
   "metadata": {},
   "source": [
    "# Transformer - Attention is all you need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7e2b4",
   "metadata": {},
   "source": [
    "## Máscaras en el Transformer: Padding y Causal Mask\n",
    "\n",
    "### ¿Por qué se necesitan máscaras?\n",
    "\n",
    "En un modelo Transformer, las **máscaras** se usan para controlar qué partes de la secuencia pueden \"verse\" entre sí durante la auto-atención. Esto es fundamental tanto en el **encoder** como en el **decoder**, pero por diferentes razones:\n",
    "\n",
    "- **Encoder** → necesita una **padding mask** para ignorar los tokens de relleno (`<PAD>`).\n",
    "- **Decoder** → necesita tanto:\n",
    "  - una **padding mask**,\n",
    "  - como una **máscara causal (look-ahead mask)** que impide ver tokens del futuro durante la generación.\n",
    "\n",
    "---\n",
    "\n",
    "| Tipo de Máscara         | Código / Ejemplo                                                                                     | Forma                                          | Descripción                                                                                                     |\n",
    "|-------------------------|------------------------------------------------------------------------------------------------------|------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|\n",
    "| **Padding Mask (source)**   | `(source != 0).unsqueeze(1).unsqueeze(2)`                                                           | `(batch_size, 1, 1, source_seq_len)`             | Ignora los tokens de padding (0) en el input del encoder.                                                       |\n",
    "| **Padding Mask (target)**   | `(target != 0).unsqueeze(1).unsqueeze(2)`                                                           | `(batch_size, 1, 1, target_seq_len)`             | Ignora los tokens de padding en el input del decoder.                                                           |\n",
    "| **Máscara Causal (Look-Ahead)** | `torch.tril(torch.ones(1, size, size)).bool()` <br> *(con `size = target.size(1)`)*                  | `(1, target_seq_len, target_seq_len)`            | Triangular inferior: permite que el token en posición *i* vea solo los tokens hasta la posición *i*.             |\n",
    "| **Target Mask Combinada**   | `(target != 0).unsqueeze(1).unsqueeze(2) & no_mask` <br> *(donde `no_mask` es la máscara causal)*     | `(batch_size, 1, target_seq_len, target_seq_len)` | Combina la máscara de padding y la causal para el decoder, usando broadcasting para ajustar las dimensiones.     |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3c13e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc6bf19b110>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Semilla de reproducibilidad\n",
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada882f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "665f1714",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93a62e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device) # filas: max_seq_len, columnas: d_model\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0)/d_model))\n",
    "        \n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Broadcasting automático\n",
    "        # x: (seq_len, batch_size, d_model)\n",
    "        # pos_embed_matrix: (seq_len, d_model)\n",
    "        # resultado: (seq_len, batch_size, d_model) + (seq_len, 1, d_model) = (seq_len, batch_size, d_model)\n",
    "        return x + self.pos_embed_matrix[:x.size(0), :]\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8): # d_model tiene que ser divisible entre num_heads. d_v = 512/8 = 64. (8*64=512). Siendo 512 el tamaño del embedding y la concatenación de las 8 cabezas igual al tamaño del embedding\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
    "        \n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model) # En lugar de hacer 8 de 512x64 hacemos una de 512x512 (más eficiente)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        '''\n",
    "        Q, K, V -> [batch_size, seq_len, num_heads*d_k]\n",
    "        Después de view Q: (batch_size, 10, 8, 64)\n",
    "        Luego de transpose se reorganiza a (batch_size, 8, 10, 64) para aplicar atención\n",
    "        '''\n",
    "        Q = self.W_q(Q).view(batch_size, -1, num_heads, d_k).transpose(1,2) # Partimos la dimension de 512 en 8 cabezas de 64. Cada token tiene 8 sub-vectores de 64 → 1 por cabeza\n",
    "        K = self.W_k(K).view(batch_size, -1, num_heads, d_k).transpose(1,2)\n",
    "        V = self.W_v(V).view(batch_size, -1, num_heads, d_k).transpose(1,2)\n",
    "        \n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "        \n",
    "    def scale_dot_product(self, Q, K, V, mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None: # En el Encoder para el padding, en el Decoder para el padding y para no ver el futuro del output\n",
    "            scores = scores.masked_fill(mask == 0, -1e9) # Para que al aplicar softmax den probabilidades de 0\n",
    "            # scores.shape = (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        attention = F.softmax(scores, dim=-1) # dim=-1 normaliza por filas\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "    \n",
    "class PositionFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class EncoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d__model, num_heads)\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_score, _ = self.self_attn(x, x, x, mask)\n",
    "        \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([]) # Nx capas secuenciales\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # mask para el padding\n",
    "        pass\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
    "        # cross-attention\n",
    "        # Necesitamos el encoder_mask para no atender a los maskings\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae262da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, input_vocab_size, target_vocab_size,\n",
    "                max_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        # d_model: Tamaño de los embeddings\n",
    "        # num_heads: Número de cabezas paralelas de atención\n",
    "        # d_ff: Tamaño de las redes neuronales Feed-Forward\n",
    "        # num_layers: Número de capas secuenciales tanto para el encoder como para el decoder\n",
    "        # input_vocab_size\n",
    "        # target_vocab_size\n",
    "        # max_len: Tamaño de la ventana de contexto\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        # Encoder mask\n",
    "        sorce_mask, target_mask = self.mask(source, target)\n",
    "        # Embedding and positional Encoding\n",
    "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim) # Técnica de escalado para normalizar los valores de los embeddings\n",
    "        source = self.pos_embedding(source)\n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        \n",
    "        # Decoder embedding and positional encoding\n",
    "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
    "        target = self.pos_embedding(target)\n",
    "        # Decoder\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "        \n",
    "        return output_layer(output)\n",
    "        \n",
    "    def mask(self, source, target):\n",
    "        # El token de 0 es de padding\n",
    "        # El resto o bien son tokens especiales (<SOS>, <EOS>) o bien palabras (Aqui cada palabra equivale a un token)\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "        size = target.size(1)  # La dimensión 1 representa la longitud de la secuencia (max_seq_len)\n",
    "        no_mask = torch.tril(torch.ones(1, size, size), device=device).bool() # Para evitar ver palabras futuras que aún no se han generado\n",
    "        target_mask = target_mask & no_mask # Broadcasting automático  # (B, 1, T, T)\n",
    "        return source_mask, target_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
