{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c28a03",
   "metadata": {},
   "source": [
    "# Transformer - Attention is all you need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3406c2a8",
   "metadata": {},
   "source": [
    "## Máscaras en el Transformer: Padding y Causal Mask\n",
    "\n",
    "### ¿Por qué se necesitan máscaras?\n",
    "\n",
    "En un modelo Transformer, las **máscaras** se usan para controlar qué partes de la secuencia pueden \"verse\" entre sí durante la auto-atención. Esto es fundamental tanto en el **encoder** como en el **decoder**, pero por diferentes razones:\n",
    "\n",
    "- **Encoder** → necesita una **padding mask** para ignorar los tokens de relleno (`<PAD>`).\n",
    "- **Decoder** → necesita tanto:\n",
    "  - una **padding mask**,\n",
    "  - como una **máscara causal (look-ahead mask)** que impide ver tokens del futuro durante la generación.\n",
    "\n",
    "---\n",
    "\n",
    "| Tipo de Máscara         | Código / Ejemplo                                                                                     | Forma                                          | Descripción                                                                                                     |\n",
    "|-------------------------|------------------------------------------------------------------------------------------------------|------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|\n",
    "| **Padding Mask (source)**   | `(source != 0).unsqueeze(1).unsqueeze(2)`                                                           | `(batch_size, 1, 1, source_seq_len)`             | Ignora los tokens de padding (0) en el input del encoder.                                                       |\n",
    "| **Padding Mask (target)**   | `(target != 0).unsqueeze(1).unsqueeze(2)`                                                           | `(batch_size, 1, 1, target_seq_len)`             | Ignora los tokens de padding en el input del decoder.                                                           |\n",
    "| **Máscara Causal (Look-Ahead)** | `torch.tril(torch.ones(1, size, size)).bool()` <br> *(con `size = target.size(1)`)*                  | `(1, target_seq_len, target_seq_len)`            | Triangular inferior: permite que el token en posición *i* vea solo los tokens hasta la posición *i*.             |\n",
    "| **Target Mask Combinada**   | `(target != 0).unsqueeze(1).unsqueeze(2) & no_mask` <br> *(donde `no_mask` es la máscara causal)*     | `(batch_size, 1, target_seq_len, target_seq_len)` | Combina la máscara de padding y la causal para el decoder, usando broadcasting para ajustar las dimensiones.     |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1fc58af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc6bf19b110>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Semilla de reproducibilidad\n",
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386ced36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df265fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa82977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device) # filas: max_seq_len, columnas: d_model\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0)/d_model))\n",
    "        \n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Broadcasting automático\n",
    "        # x: (seq_len, batch_size, d_model)\n",
    "        # pos_embed_matrix: (seq_len, d_model)\n",
    "        # resultado: (seq_len, batch_size, d_model) + (seq_len, 1, d_model) = (seq_len, batch_size, d_model)\n",
    "        return x + self.pos_embed_matrix[:x.size(0), :]\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # mask para el padding\n",
    "        pass\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
    "        # cross-attention\n",
    "        # Necesitamos el encoder_mask para no atender a los maskings\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae902c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, input_vocab_size, target_vocab_size,\n",
    "                max_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        # d_model: Tamaño de los embeddings\n",
    "        # num_heads: Número de cabezas paralelas de atención\n",
    "        # d_ff: Tamaño de las redes neuronales Feed-Forward\n",
    "        # num_layers: Número de capas secuenciales tanto para el encoder como para el decoder\n",
    "        # input_vocab_size\n",
    "        # target_vocab_size\n",
    "        # max_len: Tamaño de la ventana de contexto\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        # Encoder mask\n",
    "        sorce_mask, target_mask = self.mask(source, target)\n",
    "        # Embedding and positional Encoding\n",
    "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim) # Técnica de escalado para normalizar los valores de los embeddings\n",
    "        source = self.pos_embedding(source)\n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        \n",
    "        # Decoder embedding and positional encoding\n",
    "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
    "        target = self.pos_embedding(target)\n",
    "        # Decoder\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "        \n",
    "        return output_layer(output)\n",
    "        \n",
    "    def mask(self, source, target):\n",
    "        # El token de 0 es de padding\n",
    "        # El resto o bien son tokens especiales (<SOS>, <EOS>) o bien palabras (Aqui cada palabra equivale a un token)\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "        size = target.size(1)  # La dimensión 1 representa la longitud de la secuencia (max_seq_len)\n",
    "        no_mask = torch.tril(torch.ones(1, size, size), device=device).bool() # Para evitar ver palabras futuras que aún no se han generado\n",
    "        target_mask = target_mask & no_mask # Broadcasting automático  # (B, 1, T, T)\n",
    "        return source_mask, target_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
